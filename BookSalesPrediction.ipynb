{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd517a72-e12e-43b6-9caf-dcc26a4ad225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1070 entries, 0 to 1069\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   index                1070 non-null   int64  \n",
      " 1   Publishing Year      1069 non-null   float64\n",
      " 2   Book Name            1047 non-null   object \n",
      " 3   Author               1070 non-null   object \n",
      " 4   language_code        1017 non-null   object \n",
      " 5   Author_Rating        1070 non-null   object \n",
      " 6   Book_average_rating  1070 non-null   float64\n",
      " 7   Book_ratings_count   1070 non-null   int64  \n",
      " 8   genre                1070 non-null   object \n",
      " 9   gross sales          1070 non-null   float64\n",
      " 10  publisher revenue    1070 non-null   float64\n",
      " 11  sale price           1070 non-null   float64\n",
      " 12  sales rank           1070 non-null   int64  \n",
      " 13  Publisher            1070 non-null   object \n",
      " 14  units sold           1070 non-null   int64  \n",
      "dtypes: float64(5), int64(4), object(6)\n",
      "memory usage: 125.5+ KB\n",
      "Columns with NaN values: ['Publishing Year', 'language_code']\n",
      "|   iter    |  target   | max_depth | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Books_Data_Clean.csv\")\n",
    "\n",
    "# Analyse data\n",
    "df.info()\n",
    "\n",
    "# Pre-process data\n",
    "# Drop redundant columns \n",
    "columns_to_drop = ['index', 'Book Name', 'gross sales', 'publisher revenue', 'sales rank']\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "#df\n",
    "# Get NaN columns\n",
    "nan_columns = df.columns[df.isna().any()].tolist()\n",
    "print(\"Columns with NaN values:\", nan_columns)\n",
    "\n",
    "# Convert `Author_Rating` to numeric if it's non-numeric\n",
    "df['Author_Rating'] = pd.to_numeric(df['Author_Rating'], errors='coerce')\n",
    "\n",
    "# Define feature columns and target column\n",
    "numeric_features = ['Publishing Year', 'Author_Rating', 'Book_average_rating', 'Book_ratings_count', 'sale price']\n",
    "categorical_features = ['Author', 'language_code', 'genre']\n",
    "target = 'units sold'\n",
    "\n",
    "# Split features and target\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df[target]\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine the pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data and split - 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define objective functions for each model using Bayesian Optimisation\n",
    "\n",
    "# 1. Linear Regression\n",
    "def lr_cv():\n",
    "    lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', LinearRegression())])\n",
    "    lr_pipeline.fit(X_train, y_train)\n",
    "    y_pred = lr_pipeline.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return -rmse  # Return negative RMSE\n",
    "\n",
    "# 2. Random Forest\n",
    "def rf_cv(n_estimators, max_depth, min_samples_split):\n",
    "    rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('model', RandomForestRegressor(\n",
    "                                      n_estimators=int(n_estimators), \n",
    "                                      max_depth=int(max_depth), \n",
    "                                      min_samples_split=int(min_samples_split),\n",
    "                                      random_state=42))])\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    y_pred = rf_pipeline.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return -rmse  # Return negative RMSE\n",
    "\n",
    "# 3. XGBoost\n",
    "def xgb_cv(n_estimators, max_depth, learning_rate, subsample, colsample_bytree):\n",
    "    xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('model', xgb.XGBRegressor(\n",
    "                                       n_estimators=int(n_estimators), \n",
    "                                       max_depth=int(max_depth),\n",
    "                                       learning_rate=learning_rate,\n",
    "                                       subsample=subsample,\n",
    "                                       colsample_bytree=colsample_bytree,\n",
    "                                       random_state=42,\n",
    "                                       objective='reg:squarederror'))])\n",
    "    xgb_pipeline.fit(X_train, y_train)\n",
    "    y_pred = xgb_pipeline.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return -rmse  # Return negative RMSE\n",
    "\n",
    "# 4. SVM\n",
    "def svm_cv(C, epsilon):\n",
    "    svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('model', SVR(C=C, epsilon=epsilon))])\n",
    "    svm_pipeline.fit(X_train, y_train)\n",
    "    y_pred = svm_pipeline.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return -rmse  # Return negative RMSE\n",
    "\n",
    "# Bayesian Optimisation parameters for each model\n",
    "\n",
    "# Random Forest\n",
    "param_bounds_rf = {\n",
    "    'n_estimators': (50, 200),\n",
    "    'max_depth': (5, 20),\n",
    "    'min_samples_split': (2, 10)\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "param_bounds_xgb = {\n",
    "    'n_estimators': (50, 200),\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0)\n",
    "}\n",
    "\n",
    "# SVM\n",
    "param_bounds_svm = {\n",
    "    'C': (0.1, 10),\n",
    "    'epsilon': (0.01, 1.0)\n",
    "}\n",
    "\n",
    "# Initialise Bayesian Optimiser for each model and run optimisation (Apart from Linear Regression which has no hyperparrameters to tune.)\n",
    "\n",
    "# Random Forest\n",
    "optimizer_rf = BayesianOptimization(f=rf_cv, pbounds=param_bounds_rf, random_state=42, verbose=2)\n",
    "optimizer_rf.maximize(init_points=5, n_iter=20)\n",
    "best_params_rf = optimizer_rf.max['params']\n",
    "\n",
    "# XGBoost\n",
    "optimizer_xgb = BayesianOptimization(f=xgb_cv, pbounds=param_bounds_xgb, random_state=42, verbose=2)\n",
    "optimizer_xgb.maximize(init_points=5, n_iter=20)\n",
    "best_params_xgb = optimizer_xgb.max['params']\n",
    "\n",
    "# SVM\n",
    "optimizer_svm = BayesianOptimization(f=svm_cv, pbounds=param_bounds_svm, random_state=42, verbose=2)\n",
    "optimizer_svm.maximize(init_points=5, n_iter=20)\n",
    "best_params_svm = optimizer_svm.max['params']\n",
    "\n",
    "# Evaluate each model using the best hyperparameters\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', LinearRegression())])\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "# 2. Random Forest\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', RandomForestRegressor(\n",
    "                                  n_estimators=int(best_params_rf['n_estimators']), \n",
    "                                  max_depth=int(best_params_rf['max_depth']), \n",
    "                                  min_samples_split=int(best_params_rf['min_samples_split']),\n",
    "                                  random_state=42))])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# 3. XGBoost\n",
    "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', xgb.XGBRegressor(\n",
    "                                   n_estimators=int(best_params_xgb['n_estimators']), \n",
    "                                   max_depth=int(best_params_xgb['max_depth']),\n",
    "                                   learning_rate=best_params_xgb['learning_rate'],\n",
    "                                   subsample=best_params_xgb['subsample'],\n",
    "                                   colsample_bytree=best_params_xgb['colsample_bytree'],\n",
    "                                   random_state=42,\n",
    "                                   objective='reg:squarederror'))])\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "xgb_r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# 4. Support Vector Regressor (SVM)\n",
    "svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', SVR(C=best_params_svm['C'], epsilon=best_params_svm['epsilon']))])\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "svm_rmse = np.sqrt(mean_squared_error(y_test, y_pred_svm))\n",
    "svm_r2 = r2_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print the results with RMSE and R² score\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}, R²: {lr_r2:.4f}\")\n",
    "print(f\"Random Forest RMSE: {rf_rmse:.4f}, R²: {rf_r2:.4f}\")\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.4f}, R²: {xgb_r2:.4f}\")\n",
    "print(f\"SVM RMSE: {svm_rmse:.4f}, R²: {svm_r2:.4f}\")\n",
    "\n",
    "# Choose the best model based on RMSE\n",
    "best_model = min([(\"Linear Regression\", lr_rmse, lr_r2), \n",
    "                  (\"Random Forest\", rf_rmse, rf_r2), \n",
    "                  (\"XGBoost\", xgb_rmse, xgb_r2), \n",
    "                  (\"SVM\", svm_rmse, svm_r2)], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nBest Model: {best_model[0]} with RMSE: {best_model[1]:.4f} and R²: {best_model[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da5dc4-e53e-486f-9365-de8c0402ba84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
